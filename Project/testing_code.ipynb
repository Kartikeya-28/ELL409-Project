{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-24T07:35:04.862706Z",
     "iopub.status.busy": "2024-11-24T07:35:04.862181Z",
     "iopub.status.idle": "2024-11-24T07:35:05.909735Z",
     "shell.execute_reply": "2024-11-24T07:35:05.908629Z",
     "shell.execute_reply.started": "2024-11-24T07:35:04.862666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing the dataset\n",
    "dataset = pd.read_csv('/kaggle/input/spam-or-not-spam-dataset7/spam_or_not_spam.csv')\n",
    "texts = dataset['email'].astype(str)\n",
    "labels = dataset['label']\n",
    "\n",
    "NUM_WORDS = 20000\n",
    "MAX_LENGTH = 200 \n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, n_layers_E, n_hidden_E, dim_z):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers_E = n_layers_E\n",
    "        self.n_hidden_E = n_hidden_E\n",
    "        self.lstm = nn.LSTM(embed_dim, n_hidden_E, n_layers_E, batch_first=True, bidirectional=True)\n",
    "        self.hidden_to_mu = nn.Linear(2 * n_hidden_E, dim_z)\n",
    "        self.hidden_to_logvar = nn.Linear(2 * n_hidden_E, dim_z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        e_hidden = out[:, -1, :]  # Take the last hidden state\n",
    "        mu = self.hidden_to_mu(e_hidden)\n",
    "        logvar = self.hidden_to_logvar(e_hidden)\n",
    "        epsilon = torch.randn_like(mu)\n",
    "        z = mu + torch.exp(logvar * 0.5) * epsilon\n",
    "        return mu, logvar, z\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_hidden_D, n_layers_D, embedding_dim, dim_z, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim + dim_z, n_hidden_D, n_layers_D, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden_D, vocab_size)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        z_expanded = z.unsqueeze(1).repeat(1, seq_len, 1)  # Expand z across sequence length\n",
    "        x = torch.cat([x, z_expanded], dim=2)\n",
    "        out, _ = self.lstm(x)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, n_layers_E, n_hidden_E, dim_z, n_hidden_D, n_layers_D):\n",
    "        super(VAE, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = Encoder(embedding_dim, vocab_size, n_layers_E, n_hidden_E, dim_z)\n",
    "        self.decoder = Decoder(n_hidden_D, n_layers_D, embedding_dim, dim_z, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x)\n",
    "        mu, logvar, z = self.encoder(x_embed)\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        logits = self.decoder(x_embed, z)\n",
    "        return logits, kld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T07:35:09.980250Z",
     "iopub.status.busy": "2024-11-24T07:35:09.979775Z",
     "iopub.status.idle": "2024-11-24T07:35:11.593462Z",
     "shell.execute_reply": "2024-11-24T07:35:11.592202Z",
     "shell.execute_reply.started": "2024-11-24T07:35:09.980213Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text 1:\n",
      "Congratulations! You have been selected as a winner for a free iPhone. Click the link to claim.\n",
      "\n",
      "Reconstructed Text:\n",
      "congratulations you have been selected as a winner for a free <OOV> click the link to claim <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
      "--------------------------------------------------\n",
      "Original Text 2:\n",
      "Dear team, please find the attached report for your review and feedback.\n",
      "\n",
      "Reconstructed Text:\n",
      "dear team please find the attached report for your review and feedback <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
      "--------------------------------------------------\n",
      "Original Text 3:\n",
      "Your account has been flagged for suspicious activity. Please verify your details immediately.\n",
      "\n",
      "Reconstructed Text:\n",
      "your account has been flagged for suspicious activity please verify your details immediately <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
      "--------------------------------------------------\n",
      "Original Text 4:\n",
      "Meeting rescheduled to tomorrow at 10 AM. Let me know if this works.\n",
      "\n",
      "Reconstructed Text:\n",
      "meeting <OOV> to tomorrow at <OOV> am let me know if this works <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
      "--------------------------------------------------\n",
      "Original Text 5:\n",
      "Don't miss out on this limited-time offer to save 50% on your favorite products!\n",
      "\n",
      "Reconstructed Text:\n",
      "<OOV> miss out on this limited time offer to save <OOV> on your favorite products <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vae = pickle.load(open('/kaggle/input/notebook8589762367/VAE_LSTM_model', 'rb')) # version 9\n",
    "vae.eval()  # Switching the VAE to evaluation mode\n",
    "\n",
    "# Custom examples for testing\n",
    "custom_examples = [\n",
    "    \"Congratulations! You have been selected as a winner for a free iPhone. Click the link to claim.\",\n",
    "    \"Dear team, please find the attached report for your review and feedback.\",\n",
    "    \"Your account has been flagged for suspicious activity. Please verify your details immediately.\",\n",
    "    \"Meeting rescheduled to tomorrow at 10 AM. Let me know if this works.\",\n",
    "    \"Don't miss out on this limited-time offer to save 50% on your favorite products!\",\n",
    "]\n",
    "\n",
    "# Converting the custom examples to padded sequences using the tokenizer\n",
    "MAX_LENGTH = 200  \n",
    "sequences = tokenizer.texts_to_sequences(custom_examples)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "custom_input_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "# Passing each custom example through the VAE\n",
    "with torch.no_grad():\n",
    "    # Encoding step\n",
    "    mu, logvar, z = vae.encoder(vae.embedding(custom_input_tensor))\n",
    "    # Decoding step\n",
    "    reconstructed_logits = vae.decoder(vae.embedding(custom_input_tensor), z)\n",
    "\n",
    "# Decoding original and reconstructed sequences\n",
    "original_texts = tokenizer.sequences_to_texts(custom_input_tensor.cpu().numpy())\n",
    "reconstructed_sequences = reconstructed_logits.argmax(dim=-1).cpu().numpy()\n",
    "reconstructed_texts = tokenizer.sequences_to_texts(reconstructed_sequences)\n",
    "\n",
    "# results for each custom example\n",
    "for i, (original, reconstructed) in enumerate(zip(custom_examples, reconstructed_texts)):\n",
    "    print(f\"Original Text {i + 1}:\")\n",
    "    print(original)\n",
    "    print(\"\\nReconstructed Text:\")\n",
    "    print(reconstructed)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T07:35:16.452179Z",
     "iopub.status.busy": "2024-11-24T07:35:16.451703Z",
     "iopub.status.idle": "2024-11-24T07:35:16.460406Z",
     "shell.execute_reply": "2024-11-24T07:35:16.458909Z",
     "shell.execute_reply.started": "2024-11-24T07:35:16.452138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_data(vae, tokenizer, num_samples=10, max_length=200):\n",
    "    # Ensuring the model is in evaluation mode\n",
    "    vae.eval()\n",
    "\n",
    "    # Sample random latent vectors from a standard Gaussian distribution\n",
    "    z_random = torch.randn(num_samples, vae.encoder.hidden_to_mu.out_features).to(torch.device('cpu'))\n",
    "\n",
    "    # Creating a dummy input sequence filled with padding tokens (e.g., index 0)\n",
    "    dummy_input = torch.zeros((num_samples, max_length), dtype=torch.long).to(torch.device('cpu'))\n",
    "\n",
    "    # the embedded inputs for the dummy input\n",
    "    embedded_input = vae.embedding(dummy_input)\n",
    "\n",
    "    # Generate synthetic sequences using the decoder\n",
    "    with torch.no_grad():  # No gradient computation needed\n",
    "        synthetic_logits = vae.decoder(embedded_input, z_random)\n",
    "        synthetic_sequences = synthetic_logits.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    # Decoding synthetic sequences to text using the tokenizer\n",
    "    synthetic_texts = tokenizer.sequences_to_texts(synthetic_sequences)\n",
    "    return synthetic_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T07:35:19.522415Z",
     "iopub.status.busy": "2024-11-24T07:35:19.521934Z",
     "iopub.status.idle": "2024-11-24T07:36:06.046121Z",
     "shell.execute_reply": "2024-11-24T07:36:06.044990Z",
     "shell.execute_reply.started": "2024-11-24T07:35:19.522367Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "def calculate_reconstruction_accuracy(original_sequences, reconstructed_logits):\n",
    "    # Get reconstructed sequences from logits\n",
    "    reconstructed_sequences = reconstructed_logits.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    # Calculating token-level accuracy\n",
    "    correct_tokens = 0\n",
    "    total_tokens = 0\n",
    "    for original, reconstructed in zip(original_sequences, reconstructed_sequences):\n",
    "        for orig_token, rec_token in zip(original, reconstructed):\n",
    "            if orig_token != 0:  # Ignore padding tokens\n",
    "                total_tokens += 1\n",
    "                if orig_token == rec_token:\n",
    "                    correct_tokens += 1\n",
    "\n",
    "    return correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "\n",
    "# Evaluating reconstruction accuracy\n",
    "with torch.no_grad():\n",
    "    mu, logvar, z = vae.encoder(vae.embedding(X_test_tensor))\n",
    "    reconstructed_logits = vae.decoder(vae.embedding(X_test_tensor), z)\n",
    "\n",
    "reconstruction_accuracy = calculate_reconstruction_accuracy(X_test_tensor.cpu().numpy(), reconstructed_logits)\n",
    "print(f\"Reconstruction Accuracy: {reconstruction_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T07:36:06.048959Z",
     "iopub.status.busy": "2024-11-24T07:36:06.048475Z",
     "iopub.status.idle": "2024-11-24T07:36:38.812538Z",
     "shell.execute_reply": "2024-11-24T07:36:38.811416Z",
     "shell.execute_reply.started": "2024-11-24T07:36:06.048905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KL Divergence: 0.0001\n"
     ]
    }
   ],
   "source": [
    "def calculate_kl_divergence(mu, logvar):\n",
    "    # KL Divergence formula: 0.5 * sum(1 + logvar - mu^2 - exp(logvar))\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=-1)\n",
    "    return kl_div.mean().item()\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu, logvar, _ = vae.encoder(vae.embedding(X_test_tensor))\n",
    "\n",
    "kl_divergence = calculate_kl_divergence(mu, logvar)\n",
    "print(f\"Average KL Divergence: {kl_divergence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T07:36:38.814249Z",
     "iopub.status.busy": "2024-11-24T07:36:38.813898Z",
     "iopub.status.idle": "2024-11-24T07:36:38.865199Z",
     "shell.execute_reply": "2024-11-24T07:36:38.864106Z",
     "shell.execute_reply.started": "2024-11-24T07:36:38.814215Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Synthetic Data:\n",
      "1: nomination nomination nomination facers facers facers facers facers facers facers\n",
      "2: scenario scenario scenario consommation consommation consommation consommation consommation consommation consommation\n",
      "3: trigger trigger trigger trigger trigger trigger trigger trigger trigger trigger\n",
      "4: bello bombs orgel making making making making making making making\n",
      "5: apparently dallin dallin dallin dallin dallin dallin dallin dallin dallin\n",
      "6: graphical graphical graphical graphical graphical graphical graphical graphical graphical graphical\n",
      "7: benches benches benches benches benches benches benches benches benches benches\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the model\n",
    "embedding_dim = 64\n",
    "vocab_size = NUM_WORDS\n",
    "n_layers_E = 1\n",
    "n_hidden_E = 128\n",
    "dim_z = 32\n",
    "n_hidden_D = 128\n",
    "n_layers_D = 1\n",
    "\n",
    "vae = VAE(embedding_dim, vocab_size, n_layers_E, n_hidden_E, dim_z, n_hidden_D, n_layers_D)\n",
    "\n",
    "# Generating synthetic spam and non-spam data\n",
    "synthetic_data = generate_synthetic_data(vae, tokenizer, num_samples=7, max_length=10)\n",
    "print(\"Generated Synthetic Data:\")\n",
    "for i, text in enumerate(synthetic_data, 1):\n",
    "    print(f\"{i}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6153253,
     "sourceId": 9997390,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 209288120,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
