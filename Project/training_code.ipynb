{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16d8a1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-24T05:10:41.568735Z",
     "iopub.status.busy": "2024-11-24T05:10:41.568420Z",
     "iopub.status.idle": "2024-11-24T06:24:18.666430Z",
     "shell.execute_reply": "2024-11-24T06:24:18.665468Z"
    },
    "papermill": {
     "duration": 4417.106799,
     "end_time": "2024-11-24T06:24:18.672138",
     "exception": false,
     "start_time": "2024-11-24T05:10:41.565339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Epoch 1, Loss: 432.9745\n",
      "Test Loss: 4.2604\n",
      "Epoch 2, Loss: 5162.5807\n",
      "Test Loss: 3.6167\n",
      "Epoch 3, Loss: 239.1407\n",
      "Test Loss: 2.8100\n",
      "Epoch 4, Loss: 185.2234\n",
      "Test Loss: 2.2109\n",
      "Epoch 5, Loss: 146.6892\n",
      "Test Loss: 1.7738\n",
      "Epoch 6, Loss: 119.2183\n",
      "Test Loss: 1.4627\n",
      "Epoch 7, Loss: 99.1529\n",
      "Test Loss: 1.2350\n",
      "Epoch 8, Loss: 84.2666\n",
      "Test Loss: 1.0623\n",
      "Epoch 9, Loss: 72.9130\n",
      "Test Loss: 0.9290\n",
      "Epoch 10, Loss: 64.0935\n",
      "Test Loss: 0.8242\n",
      "Epoch 11, Loss: 56.9486\n",
      "Test Loss: 0.7390\n",
      "Epoch 12, Loss: 51.2804\n",
      "Test Loss: 0.6709\n",
      "Epoch 13, Loss: 46.6876\n",
      "Test Loss: 0.6140\n",
      "Epoch 14, Loss: 42.9275\n",
      "Test Loss: 0.5670\n",
      "Epoch 15, Loss: 39.8322\n",
      "Test Loss: 0.5308\n",
      "Epoch 16, Loss: 37.2253\n",
      "Test Loss: 0.4952\n",
      "Epoch 17, Loss: 35.0335\n",
      "Test Loss: 0.4688\n",
      "Epoch 18, Loss: 33.1731\n",
      "Test Loss: 0.4446\n",
      "Epoch 19, Loss: 31.5638\n",
      "Test Loss: 0.4246\n",
      "Epoch 20, Loss: 30.1642\n",
      "Test Loss: 0.4064\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing the dataset\n",
    "dataset = pd.read_csv('/kaggle/input/spam-dataset/spam_or_not_spam.csv')\n",
    "texts = dataset['email'].astype(str)\n",
    "labels = dataset['label']\n",
    "\n",
    "NUM_WORDS = 20000\n",
    "MAX_LENGTH = 200  # Truncate or pad sequences to this length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n",
    "\n",
    "# defining the encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, n_layers_E, n_hidden_E, dim_z):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers_E = n_layers_E\n",
    "        self.n_hidden_E = n_hidden_E\n",
    "        self.lstm = nn.LSTM(embed_dim, n_hidden_E, n_layers_E, batch_first=True, bidirectional=True)\n",
    "        self.hidden_to_mu = nn.Linear(2 * n_hidden_E, dim_z)\n",
    "        self.hidden_to_logvar = nn.Linear(2 * n_hidden_E, dim_z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        e_hidden = out[:, -1, :]  # Take the last hidden state\n",
    "        mu = self.hidden_to_mu(e_hidden)\n",
    "        logvar = self.hidden_to_logvar(e_hidden)\n",
    "        epsilon = torch.randn_like(mu)\n",
    "        z = mu + torch.exp(logvar * 0.5) * epsilon\n",
    "        return mu, logvar, z\n",
    "\n",
    "# defining the decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_hidden_D, n_layers_D, embedding_dim, dim_z, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim + dim_z, n_hidden_D, n_layers_D, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden_D, vocab_size)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        z_expanded = z.unsqueeze(1).repeat(1, seq_len, 1)  # Expand z across sequence length\n",
    "        x = torch.cat([x, z_expanded], dim=2)\n",
    "        out, _ = self.lstm(x)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "# Variational Autoencoder\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, n_layers_E, n_hidden_E, dim_z, n_hidden_D, n_layers_D):\n",
    "        super(VAE, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = Encoder(embedding_dim, vocab_size, n_layers_E, n_hidden_E, dim_z)\n",
    "        self.decoder = Decoder(n_hidden_D, n_layers_D, embedding_dim, dim_z, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x)\n",
    "        mu, logvar, z = self.encoder(x_embed)\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        logits = self.decoder(x_embed, z)\n",
    "        return logits, kld\n",
    "\n",
    "# Preparing DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data=TensorDataset(X_test_tensor,y_test_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initializing the VAE\n",
    "EMBEDDING_DIM = 64\n",
    "N_LAYERS_E = 1\n",
    "N_HIDDEN_E = 128\n",
    "DIM_Z = 32\n",
    "N_HIDDEN_D = 128\n",
    "N_LAYERS_D = 1\n",
    "\n",
    "vae = VAE(EMBEDDING_DIM, NUM_WORDS, N_LAYERS_E, N_HIDDEN_E, DIM_Z, N_HIDDEN_D, N_LAYERS_D)\n",
    "\n",
    "# Saving model to a directory\n",
    "import pickle\n",
    "pickle.dump(vae, open('/kaggle/working/VAE_LSTM_model_init', 'wb'))\n",
    "\n",
    "# Training the VAE\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "print(\"start training\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    vae.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_seq, _ = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, kld = vae(input_seq)\n",
    "        logits_reshaped = logits.view(-1, NUM_WORDS)\n",
    "        input_seq_reshaped = input_seq.view(-1)\n",
    "        \n",
    "        reconstruction_loss = criterion(logits_reshaped, input_seq_reshaped)\n",
    "        beta = min(0.9, epoch/10)  # KL annealing\n",
    "        total_loss = reconstruction_loss + beta * kld\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += total_loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    vae.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    for batch in test_loader:\n",
    "            input_seq, labels = batch\n",
    "            logits, kld = vae(input_seq)\n",
    "            logits_reshaped = logits.view(-1, NUM_WORDS)\n",
    "            input_seq_reshaped = input_seq.view(-1)\n",
    "            reconstruction_loss = criterion(logits_reshaped, input_seq_reshaped)\n",
    "            beta = min(0.9, epoch/10)  # KL annealing\n",
    "            loss = reconstruction_loss + beta * kld\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Saving model to a directory\n",
    "pickle.dump(vae, open('/kaggle/working/VAE_LSTM_model', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6150940,
     "sourceId": 9993834,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4422.869273,
   "end_time": "2024-11-24T06:24:21.855009",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-24T05:10:38.985736",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
